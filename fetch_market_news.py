"""
Step 1.5: ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘
Jina Search & Reader APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œì¥ ë° ì„¹í„° ë‰´ìŠ¤ ìˆ˜ì§‘
yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
"""

import os
import sys
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import re
from dotenv import load_dotenv
import yfinance as yf
import xml.etree.ElementTree as ET

# Windows í™˜ê²½ì—ì„œ UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class MarketNewsCollector:
    """Jina AIë¥¼ ì‚¬ìš©í•œ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘ í´ë˜ìŠ¤"""

    # RSS Feed URLs
    KAGI_BUSINESS_RSS_URL = "https://news.kagi.com/business.xml"
    CNBC_STOCK_NEWS_RSS_URL = "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664"
    NASDAQ_STOCK_RSS_URL_TEMPLATE = "https://www.nasdaq.com/feed/rssoutbound?symbol={symbol}"

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("JINA_API_KEY")
        self.has_jina_api = bool(self.api_key)

        if not self.has_jina_api:
            print("âš ï¸ JINA_API_KEY not found - will skip market/sector news collection")
            print("   Only stock-specific news (via yfinance) will be collected")

        self.search_url = "https://s.jina.ai/"
        self.reader_url = "https://r.jina.ai/"

    def parse_date_to_standard(self, date_str: str) -> str:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹(YYYY-MM-DD HH:MM:SS)ìœ¼ë¡œ ë³€í™˜"""
        if not date_str or date_str == 'unknown':
            return '1900-01-01 00:00:00'

        try:
            now = datetime.now()

            # ìƒëŒ€ ì‹œê°„ ì²˜ë¦¬: "4 hours ago", "1 day ago"
            if 'ago' in date_str.lower():
                numbers = re.findall(r'\d+', date_str)
                if not numbers:
                    return now.strftime('%Y-%m-%d %H:%M:%S')

                value = int(numbers[0])
                if 'hour' in date_str.lower():
                    target_date = now - timedelta(hours=value)
                elif 'day' in date_str.lower():
                    target_date = now - timedelta(days=value)
                elif 'week' in date_str.lower():
                    target_date = now - timedelta(weeks=value)
                elif 'month' in date_str.lower():
                    target_date = now - timedelta(days=value * 30)
                else:
                    target_date = now
                return target_date.strftime('%Y-%m-%d %H:%M:%S')

            # ISO 8601 í˜•ì‹: "2025-10-01T08:19:28+00:00"
            if 'T' in date_str:
                date_part = date_str.split('+')[0].split('-')[0:3]
                date_part = '-'.join(date_part[:3])
                if len(date_part.split('T')) > 1:
                    parsed_date = datetime.strptime(date_part, '%Y-%m-%dT%H:%M:%S')
                else:
                    parsed_date = datetime.strptime(date_part, '%Y-%m-%d')
                return parsed_date.strftime('%Y-%m-%d %H:%M:%S')

            # ì¼ë°˜ í˜•ì‹: "May 31, 2025" ë˜ëŠ” "2025-05-31"
            if ',' in date_str:
                parsed_date = datetime.strptime(date_str.strip(), '%b %d, %Y')
            elif '-' in date_str and len(date_str) == 10:
                parsed_date = datetime.strptime(date_str, '%Y-%m-%d')
            else:
                return now.strftime('%Y-%m-%d %H:%M:%S')

            return parsed_date.strftime('%Y-%m-%d %H:%M:%S')

        except Exception as e:
            print(f"âš ï¸ Date parsing error for '{date_str}': {e}")
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def search(self, query: str, max_results: int = 5, cutoff_date: Optional[str] = None) -> List[str]:
        """Jina Search APIë¡œ ê²€ìƒ‰ ìˆ˜í–‰"""
        url = f'{self.search_url}?q={query}&n={max_results}'
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Accept': 'application/json',
            'X-Respond-With': 'no-content'
        }

        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            filtered_urls = []
            for item in data.get('data', []):
                raw_date = item.get('date', 'unknown')
                standardized_date = self.parse_date_to_standard(raw_date)

                # ë‚ ì§œ í•„í„°ë§ (cutoff_date ì´ì „ ì •ë³´ë§Œ)
                if cutoff_date and standardized_date > cutoff_date:
                    continue

                filtered_urls.append({
                    'url': item.get('url'),
                    'title': item.get('title', 'No title'),
                    'description': item.get('description', ''),
                    'date': standardized_date
                })

            print(f"  Found {len(filtered_urls)} results for '{query}'")
            return filtered_urls

        except Exception as e:
            print(f"âš ï¸ Search error for '{query}': {e}")
            return []

    def read_url(self, url: str) -> Optional[Dict]:
        """Jina Reader APIë¡œ URL ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘"""
        reader_url = f'{self.reader_url}{url}'
        headers = {
            'Accept': 'application/json',
            'Authorization': f'Bearer {self.api_key}',
            'X-Timeout': '10',
            'X-With-Generated-Alt': 'true',
        }

        try:
            response = requests.get(reader_url, headers=headers, timeout=15)
            response.raise_for_status()
            data = response.json()

            content = data.get('data', {}).get('content', '')
            # ì»¨í…ì¸  ê¸¸ì´ ì œí•œ (ì²« 2000ì)
            if len(content) > 2000:
                content = content[:2000] + '...'

            return {
                'url': url,
                'title': data.get('data', {}).get('title', 'No title'),
                'description': data.get('data', {}).get('description', ''),
                'content': content,
                'publish_time': data.get('data', {}).get('publishedTime', 'unknown')
            }

        except Exception as e:
            print(f"âš ï¸ Reader error for '{url}': {e}")
            return None

    def fetch_cnbc_stock_news(self, cutoff_date: str, max_news: int = 10) -> List[Dict]:
        """CNBC ì£¼ì‹ ë‰´ìŠ¤ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"  Fetching CNBC stock news from RSS...")
            response = requests.get(self.CNBC_STOCK_NEWS_RSS_URL, timeout=15)
            response.raise_for_status()

            # XML íŒŒì‹±
            root = ET.fromstring(response.content)
            news_items = []

            # RSS 2.0 í˜•ì‹ íŒŒì‹±
            for item in root.findall('.//item')[:max_news]:
                title = item.find('title')
                link = item.find('link')
                description = item.find('description')
                pub_date = item.find('pubDate')

                # ë‚ ì§œ íŒŒì‹± (RFC 822 í˜•ì‹)
                publish_time = 'unknown'
                if pub_date is not None and pub_date.text:
                    try:
                        date_str = pub_date.text
                        # +0000 í˜•ì‹ì˜ timezoneì„ ì œê±°í•˜ê³  íŒŒì‹±
                        if '+0000' in date_str or '-0000' in date_str:
                            date_str = date_str.replace('+0000', '').replace('-0000', '').strip()
                            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S')
                        elif ' GMT' in date_str or ' UTC' in date_str:
                            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')
                        else:
                            # ê¸°íƒ€ í˜•ì‹ì€ ê¸°ì¡´ parse_date_to_standard ì‚¬ìš©
                            publish_time = self.parse_date_to_standard(date_str)
                            dt = None

                        if dt:
                            publish_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception as e:
                        print(f"    âš ï¸ Date parsing error: {e}")
                        publish_time = self.parse_date_to_standard(pub_date.text)

                # cutoff_date í•„í„°ë§
                if publish_time != 'unknown' and publish_time > cutoff_date:
                    continue

                news_items.append({
                    'url': link.text if link is not None else '',
                    'title': title.text if title is not None else 'No title',
                    'description': description.text if description is not None else '',
                    'content': description.text if description is not None else '',
                    'publish_time': publish_time,
                    'source': 'CNBC Stock News'
                })

            print(f"    âœ“ Found {len(news_items)} CNBC stock news items")
            return news_items

        except Exception as e:
            print(f"âš ï¸ CNBC RSS fetch error: {e}")
            return []

    def fetch_nasdaq_stock_news(self, symbol: str, cutoff_date: str, max_news: int = 5) -> List[Dict]:
        """NASDAQ RSS í”¼ë“œì—ì„œ ì¢…ëª©ë³„ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        rss_url = self.NASDAQ_STOCK_RSS_URL_TEMPLATE.format(symbol=symbol)

        try:
            # User-Agent í—¤ë” ì¶”ê°€ (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” ë´‡ ì°¨ë‹¨)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(rss_url, headers=headers, timeout=30)
            response.raise_for_status()

            # XML íŒŒì‹±
            root = ET.fromstring(response.content)
            news_items = []

            # RSS 2.0 í˜•ì‹ íŒŒì‹±
            for item in root.findall('.//item')[:max_news]:
                title = item.find('title')
                link = item.find('link')
                description = item.find('description')
                pub_date = item.find('pubDate')

                # ë‚ ì§œ íŒŒì‹± (RFC 822 í˜•ì‹)
                publish_time = 'unknown'
                if pub_date is not None and pub_date.text:
                    try:
                        date_str = pub_date.text
                        # +0000 í˜•ì‹ì˜ timezoneì„ ì œê±°í•˜ê³  íŒŒì‹±
                        if '+0000' in date_str or '-0000' in date_str:
                            date_str = date_str.replace('+0000', '').replace('-0000', '').strip()
                            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S')
                        elif ' GMT' in date_str or ' UTC' in date_str:
                            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')
                        else:
                            # ê¸°íƒ€ í˜•ì‹ì€ ê¸°ì¡´ parse_date_to_standard ì‚¬ìš©
                            publish_time = self.parse_date_to_standard(date_str)
                            dt = None

                        if dt:
                            publish_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception as e:
                        print(f"    âš ï¸ Date parsing error: {e}")
                        publish_time = self.parse_date_to_standard(pub_date.text)

                # cutoff_date í•„í„°ë§
                if publish_time != 'unknown' and publish_time > cutoff_date:
                    continue

                news_items.append({
                    'url': link.text if link is not None else '',
                    'title': title.text if title is not None else 'No title',
                    'description': description.text if description is not None else '',
                    'content': description.text if description is not None else '',
                    'publish_time': publish_time,
                    'source': f'NASDAQ - {symbol}'
                })

            return news_items

        except Exception as e:
            print(f"    âš ï¸ NASDAQ RSS fetch error for {symbol}: {e}")
            return []

    def fetch_kagi_business_news(self, cutoff_date: str, max_news: int = 10) -> List[Dict]:
        """Kagi ë¹„ì¦ˆë‹ˆìŠ¤ ë‰´ìŠ¤ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"  Fetching Kagi business news from RSS...")
            response = requests.get(self.KAGI_BUSINESS_RSS_URL, timeout=15)
            response.raise_for_status()

            # XML íŒŒì‹±
            root = ET.fromstring(response.content)
            news_items = []

            # RSS 2.0 í˜•ì‹ íŒŒì‹±
            for item in root.findall('.//item')[:max_news]:
                title = item.find('title')
                link = item.find('link')
                description = item.find('description')
                pub_date = item.find('pubDate')

                # ë‚ ì§œ íŒŒì‹± (RFC 822 í˜•ì‹: "Mon, 28 Oct 2024 12:00:00 +0000")
                publish_time = 'unknown'
                if pub_date is not None and pub_date.text:
                    try:
                        date_str = pub_date.text
                        # +0000 í˜•ì‹ì˜ timezoneì„ ì œê±°í•˜ê³  íŒŒì‹±
                        if '+0000' in date_str or '-0000' in date_str:
                            date_str = date_str.replace('+0000', '').replace('-0000', '').strip()
                            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S')
                        elif ' GMT' in date_str or ' UTC' in date_str:
                            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')
                        else:
                            # ê¸°íƒ€ í˜•ì‹ì€ ê¸°ì¡´ parse_date_to_standard ì‚¬ìš©
                            publish_time = self.parse_date_to_standard(date_str)
                            dt = None

                        if dt:
                            publish_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception as e:
                        print(f"    âš ï¸ Date parsing error: {e}")
                        publish_time = self.parse_date_to_standard(pub_date.text)

                # cutoff_date í•„í„°ë§
                if publish_time != 'unknown' and publish_time > cutoff_date:
                    continue

                news_items.append({
                    'url': link.text if link is not None else '',
                    'title': title.text if title is not None else 'No title',
                    'description': description.text if description is not None else '',
                    'content': description.text if description is not None else '',
                    'publish_time': publish_time,
                    'source': 'Kagi Business News'
                })

            print(f"    âœ“ Found {len(news_items)} business news items")
            return news_items

        except Exception as e:
            print(f"âš ï¸ Kagi RSS fetch error: {e}")
            return []

    def get_stock_news_yfinance(self, symbol: str, max_news: int = 3) -> List[Dict]:
        """yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢…ëª© ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            ticker = yf.Ticker(symbol)
            news = ticker.news

            if not news:
                return []

            # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ë§Œ ê°€ì ¸ì˜¤ê¸°
            news_items = []
            for item in news[:max_news]:
                # ì¤‘ì²©ëœ content êµ¬ì¡° ì²˜ë¦¬
                content = item.get('content', {})

                # pubDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (ISO 8601 í˜•ì‹)
                pub_date_str = content.get('pubDate', '')
                try:
                    if pub_date_str:
                        # ISO 8601 í˜•ì‹ íŒŒì‹±: "2025-10-28T20:03:53Z"
                        pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
                        publish_time = pub_date.strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        publish_time = 'unknown'
                except Exception:
                    publish_time = 'unknown'

                # ë‰´ìŠ¤ URL
                canonical_url = content.get('canonicalUrl', {})
                url = canonical_url.get('url', '')

                # Provider ì •ë³´
                provider = content.get('provider', {})
                publisher = provider.get('displayName', 'Unknown')

                news_items.append({
                    'url': url,
                    'title': content.get('title', 'No title'),
                    'description': content.get('description', ''),
                    'content': content.get('summary', ''),  # summaryë¥¼ contentë¡œ ì‚¬ìš©
                    'publish_time': publish_time,
                    'publisher': publisher
                })

            return news_items

        except Exception as e:
            print(f"âš ï¸ yfinance news error for '{symbol}': {e}")
            return []

    def collect_market_news(self, trading_date: str, symbols: List[str]) -> Dict:
        """ì‹œì¥ ë‰´ìŠ¤ ë° ì£¼ìš” ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print(f"\nğŸ“° Collecting market news for {trading_date}...")

        # ê±°ë˜ì¼ ê¸°ì¤€ìœ¼ë¡œ cutoff (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)
        cutoff_datetime = f"{trading_date} 23:59:59"

        news_data = {
            'trading_date': trading_date,
            'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'market_overview': [],
            'sector_news': [],
            'top_stocks_news': {}
        }

        # 1. Kagi ë¹„ì¦ˆë‹ˆìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ (í•­ìƒ ì‹¤í–‰)
        print("\n1ï¸âƒ£ Collecting Kagi business news...")
        kagi_news = self.fetch_kagi_business_news(cutoff_datetime, max_news=5)
        news_data['market_overview'].extend(kagi_news)

        # 1-2. CNBC ì£¼ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘ (í•­ìƒ ì‹¤í–‰)
        print("\n1-2ï¸âƒ£ Collecting CNBC stock news...")
        cnbc_news = self.fetch_cnbc_stock_news(cutoff_datetime, max_news=5)
        news_data['market_overview'].extend(cnbc_news)

        # 2. ì „ì²´ ì‹œì¥ ë‰´ìŠ¤ (JINA API ì‚¬ìš© ê°€ëŠ¥í•  ë•Œë§Œ)
        if self.has_jina_api:
            print("\n2ï¸âƒ£ Collecting general market news...")
            market_queries = [
                "NASDAQ stock market news today",
                "US stock market outlook",
                "tech stocks market analysis"
            ]

            for query in market_queries:
                results = self.search(query, max_results=2, cutoff_date=cutoff_datetime)
                for result in results[:1]:  # ê° ì¿¼ë¦¬ë‹¹ 1ê°œì”©ë§Œ
                    article = self.read_url(result['url'])
                    if article:
                        news_data['market_overview'].append(article)

            # 3. ì„¹í„° ë‰´ìŠ¤
            print("\n3ï¸âƒ£ Collecting sector news...")
            sector_queries = [
                "technology sector stocks",
                "semiconductor industry news"
            ]

            for query in sector_queries:
                results = self.search(query, max_results=2, cutoff_date=cutoff_datetime)
                for result in results[:1]:
                    article = self.read_url(result['url'])
                    if article:
                        news_data['sector_news'].append(article)
        else:
            print("\nâ­ï¸  Skipping market/sector news (no JINA API key)")

        # 4. ì£¼ìš” ì¢…ëª© ë‰´ìŠ¤ (ì‹œê°€ì´ì•¡ ìƒìœ„ 20ê°œë§Œ) - yfinance + NASDAQ RSS ì‚¬ìš©
        print("\n4ï¸âƒ£ Collecting top stocks news (using yfinance + NASDAQ RSS)...")
        top_symbols = symbols[:20]  # ìƒìœ„ 20ê°œë§Œ

        for symbol in top_symbols:
            print(f"  Fetching news for {symbol}...")

            # yfinance ë‰´ìŠ¤
            yf_news_items = self.get_stock_news_yfinance(symbol, max_news=2)

            # NASDAQ RSS ë‰´ìŠ¤
            nasdaq_news_items = self.fetch_nasdaq_stock_news(symbol, cutoff_datetime, max_news=2)

            # ë‘ ì†ŒìŠ¤ í•©ì¹˜ê¸°
            all_news_items = yf_news_items + nasdaq_news_items

            if all_news_items:
                # cutoff_date ì´ì „ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                filtered_news = []
                for item in all_news_items:
                    if item['publish_time'] <= cutoff_datetime:
                        filtered_news.append(item)

                if filtered_news:
                    news_data['top_stocks_news'][symbol] = filtered_news
                    print(f"    âœ“ Found {len(filtered_news)} news items (yfinance: {len(yf_news_items)}, NASDAQ: {len(nasdaq_news_items)})")
                else:
                    print(f"    âš  No news within cutoff date")
            else:
                print(f"    âš  No news available")

        # í†µê³„ ì¶œë ¥
        print(f"\nâœ… News collection complete:")
        print(f"   - Market overview: {len(news_data['market_overview'])} articles")
        print(f"   - Sector news: {len(news_data['sector_news'])} articles")
        print(f"   - Stock news: {len(news_data['top_stocks_news'])} stocks")

        return news_data


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê±°ë˜ ëª¨ë“œ í™•ì¸
    use_alpaca = os.getenv("USE_ALPACA", "false").lower() == "true"
    simulation_mode = os.getenv("SIMULATION_MODE", "true").lower() == "true"

    # ê±°ë˜ ë‚ ì§œ
    trading_date = os.getenv("TRADING_DATE")
    if not trading_date:
        trading_datetime = os.getenv("TRADING_DATETIME")
        if trading_datetime:
            trading_date = trading_datetime.split('T')[0] if 'T' in trading_datetime else trading_datetime.split()[0]
        else:
            now = datetime.now()
            if now.weekday() >= 5:  # ì£¼ë§
                print("âš ï¸ Weekend - No trading")
                return
            trading_date = now.strftime("%Y-%m-%d")
    else:
        # Alpaca ëª¨ë“œì—ì„œ ê³¼ê±° ë‚ ì§œ ì…ë ¥ ì‹œ ê²½ê³  ë° ë¬´ì‹œ
        if use_alpaca and not simulation_mode:
            today = datetime.now().strftime("%Y-%m-%d")
            if trading_date != today:
                print(f"\nâš ï¸  WARNING: Alpaca mode cannot use past dates for news")
                print(f"   Requested date: {trading_date}")
                print(f"   Current date: {today}")
                print(f"   â†’ Ignoring past date and using today's date")
                print(f"   â†’ For backtesting, use SIMULATION_MODE=true\n")
                trading_date = today

    print(f"ğŸ“… Trading Date: {trading_date}")

    # NASDAQ 100 ì‹¬ë³¼ (prepare_trading_data.pyì™€ ë™ì¼)
    symbols = [
        "NVDA", "MSFT", "AAPL", "GOOG", "GOOGL", "AMZN", "META", "AVGO", "TSLA",
        "NFLX", "PLTR", "COST", "ASML", "AMD", "CSCO", "AZN", "TMUS", "MU", "LIN",
        "PEP", "SHOP", "APP", "INTU", "AMAT", "LRCX", "PDD", "QCOM", "ARM", "INTC",
        "BKNG", "AMGN", "TXN", "ISRG", "GILD", "KLAC", "PANW", "ADBE", "HON",
        "CRWD", "CEG", "ADI", "ADP", "DASH", "CMCSA", "VRTX", "MELI", "SBUX",
        "CDNS", "ORLY", "SNPS", "MSTR", "MDLZ", "ABNB", "MRVL", "CTAS", "TRI",
        "MAR", "MNST", "CSX", "ADSK", "PYPL", "FTNT", "AEP", "WDAY", "REGN", "ROP",
        "NXPI", "DDOG", "AXON", "ROST", "IDXX", "EA", "PCAR", "FAST", "EXC", "TTWO",
        "XEL", "ZS", "PAYX", "WBD", "BKR", "CPRT", "CCEP", "FANG", "TEAM", "CHTR",
        "KDP", "MCHP", "GEHC", "VRSK", "CTSH", "CSGP", "KHC", "ODFL", "DXCM", "TTD",
        "ON", "BIIB", "LULU", "CDW", "GFS"
    ]

    # ë‰´ìŠ¤ ìˆ˜ì§‘
    collector = MarketNewsCollector()
    news_data = collector.collect_market_news(trading_date, symbols)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = Path("market_news.json")
    with open(output_file, "w", encoding='utf-8') as f:
        json.dump(news_data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Market news saved to: {output_file}")


if __name__ == "__main__":
    main()
